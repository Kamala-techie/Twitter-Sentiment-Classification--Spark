{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Members: Vakkalanka V S S Dilip Raju (vvakka2), Lakshmi Kamala Kotha(lkotha2)\n",
    "Installations:\n",
    "    installed NLTK and stopwords\n",
    "    !pip install nltk\n",
    "    nltk.download()[installed stop words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below Cell will import all the neccessary libararies and will print a success message if the import is successful\n",
    "Removed punctuations, http(s),www\n",
    "Provided some regular expressions for processing the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported Spark Modules\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import string\n",
    "import csv,re,io\n",
    "import nltk\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark import SparkConf\n",
    "    from pyspark.mllib.feature import Normalizer\n",
    "    from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "    from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "    from pyspark.mllib.linalg import Vectors\n",
    "    from pyspark.mllib.regression import LabeledPoint\n",
    "    from pyspark.mllib.feature import HashingTF\n",
    "    from pyspark.mllib.feature import IDF\n",
    "    from pyspark.mllib.feature import StandardScaler\n",
    "    from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel,LogisticRegressionWithSGD\n",
    "    from pyspark.mllib.evaluation import BinaryClassificationMetrics,MulticlassMetrics\n",
    "    from pyspark.mllib.util import MLUtils \n",
    "    from pyspark.ml.feature import NGram\n",
    "    from pyspark.sql import SQLContext\n",
    "    print(\"Successfully imported Spark Modules\")\n",
    "except ImportError as e:\n",
    "    print(\"Error importing Spark Modules\", e)\n",
    "    sys.exit(1)\n",
    "\n",
    "punc = string.punctuation.replace('_', '')\n",
    "\n",
    "punc_regex = re.compile('[%s]' % re.escape(punc))\n",
    "url_regex = '(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?'\n",
    "nonalpha1 = re.compile('[0-9]+[a-z]+')\n",
    "nonalpha2 = re.compile('\\s[0-9]+\\s')\n",
    "one_two = '\\s+\\w\\w?\\s+'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre Processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below function will perform all the neccessary processing steps on the tweet and parse_line will return the tweet with its output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punc(s):\n",
    "    temp = re.sub( url_regex, 'url', s).strip()  # Remove URLs from the text\n",
    "    temp = re.sub('^@([A-Za-z0-9_]+)', 'atuser', temp).strip()  # Replace user names with AT_USER\n",
    "    temp = nonalpha1.sub(' ',temp)\n",
    "    temp = nonalpha2.sub(' ',temp)\n",
    "    temp = re.sub(r'(.)\\1{2,}',r'\\1', temp)\n",
    "    temp = punc_regex.sub(' ', temp).strip()  # Replace punctuation with out '_'\n",
    "    temp = re.sub(one_two, ' ', temp)\n",
    "    return re.sub( '\\s+', ' ', temp).strip()\n",
    "\n",
    "\n",
    "def parse_line(line):\n",
    "    columns = csv.reader(io.StringIO(line))\n",
    "    k = columns.__next__()\n",
    "    polarity = int(k[0].strip('\"'))\n",
    "    tweet = k[5].strip().lower()\n",
    "    tweet = remove_punc(tweet)\n",
    "    return tweet, polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Stopwords removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell will perform stemming and removes stop words from nltk stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STEMMER = PorterStemmer()\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def tokenizer(line):\n",
    "    tweet_words = line.strip().split(\" \")\n",
    "    tweet_nostopwords = [word for word in tweet_words if not word in STOPWORDS]\n",
    "    tweet_stemmed = [STEMMER.stem(word) for word in tweet_nostopwords]\n",
    "    return [word for word in tweet_stemmed if word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell will retreive test and train data. It will perform the above preprocessing steps and tokenizes the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Parsing Train Data...\n",
      "## Tokenizing Train Data...\n",
      "## Parsing Test Data...\n",
      "## Tokenizing Test Data...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    conf = (SparkConf().setAppName(\"TwitterSA\").set(\"spark.executor.memory\", \"6g\"))\n",
    "    #sc = SparkContext(conf=conf)\n",
    "    sc = pyspark.SparkContext('local[*]')\n",
    "    sqlContext = SQLContext(sc)\n",
    "    input_train = \"train.csv\"\n",
    "    input_test = \"test.csv\"\n",
    "\n",
    "    raw_train = sc.textFile(input_train)\n",
    "    print('## Parsing Train Data...')\n",
    "    train_data = raw_train.map(parse_line)\n",
    "    print('## Tokenizing Train Data...')\n",
    "    train_data = train_data.map(lambda x: (tokenizer(x[0]),x[1]))\n",
    "    train_data.cache()\n",
    "    # print(train_data.take(3))\n",
    "\n",
    "    raw_test = sc.textFile(input_test)\n",
    "    print('## Parsing Test Data...')\n",
    "    test_data = raw_test.map(parse_line)\n",
    "    print('## Tokenizing Test Data...')\n",
    "    test_data = test_data.map(lambda x: (tokenizer(x[0]),x[1]))\n",
    "    test_data.cache()\n",
    "    # print(test_data.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def biGramfunc(tweet):\n",
    "    wordDataFrame = sqlContext.createDataFrame(tweet, [\"words\",\"polarity\"])\n",
    "    ngram = NGram(inputCol=\"words\", outputCol=\"ngrams\")\n",
    "    ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "    #for ngrams_label in ngramDataFrame.take(3):\n",
    "    #    print(ngrams_label)\n",
    "    ngrams_rdd = ngramDataFrame.select(\"polarity\",\"ngrams\").rdd.map(lambda x:[x[1],x[0]])\n",
    "    #print(ngrams_rdd.take(3))\n",
    "    return ngrams_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used 50000 as the number of features. (We need to modify it to 4000 for running decision trees). We built labeled training and test data which contains both the class label and tweet(which is transformed into TF vector using hashtingTF))\n",
    "\n",
    "Trained the naive bayes using labeled train data and calculated both the train and test accuracy.\n",
    "\n",
    "Found different metrics using multiclass metrics and found the area of the roc curve using BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Building model on Train Data...\n",
      "[LabeledPoint(0.0, (50000,[703,2553,8539,11402,21076,21596,24604,37581,41977],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])), LabeledPoint(0.0, (50000,[2553,8539,20451,24323,25651],[1.0,1.0,1.0,1.0,1.0]))]\n",
      "Training Data 0.8041375\n",
      "Testing Data 0.8105849582172702\n",
      "[[ 143.   34.]\n",
      " [  34.  148.]]\n",
      "Summary Stats\n",
      "Precision = 0.8105849582172702\n",
      "Recall = 0.8105849582172702\n",
      "F1 Score = 0.8105849582172702\n",
      "Class 0.0 precision = 0.807909604519774\n",
      "Class 0.0 recall = 0.807909604519774\n",
      "Class 0.0 F1 Measure = 0.8079096045197741\n",
      "\n",
      "\n",
      "Class 1.0 precision = 0.8131868131868132\n",
      "Class 1.0 recall = 0.8131868131868132\n",
      "Class 1.0 F1 Measure = 0.8131868131868132\n",
      "\n",
      "\n",
      "Area under ROC =0.8105482088532936\n"
     ]
    }
   ],
   "source": [
    "    hashingTF = HashingTF(50000)\n",
    "    ### Start of Naive bayes\n",
    "\n",
    "    print('## Building model on Train Data...')\n",
    "    \n",
    "    labeled_training_data = train_data.map(lambda x: LabeledPoint(x[1],hashingTF.transform(x[0])))\n",
    "    labeled_training_data.persist()\n",
    "    print(labeled_training_data.take(2))\n",
    "    labeled_testing_data = test_data.map(lambda x: LabeledPoint(x[1],hashingTF.transform(x[0])))\n",
    "    labeled_testing_data.persist()\n",
    "    \n",
    "    NBmodel = NaiveBayes.train(labeled_training_data,lambda_=1.0)\n",
    "    \n",
    "    \n",
    "    NB_P_O_train = labeled_training_data.map(lambda p: (NBmodel.predict(p.features), p.label))\n",
    "    accuracy = 1.0 * NB_P_O_train.filter(lambda x: x[0] == x[1]).count() / labeled_training_data.count()\n",
    "    print(\"Training Data\",accuracy)\n",
    "    #\n",
    "    NB_P_O_test = labeled_testing_data.map(lambda p:(float(NBmodel.predict(p.features)), p.label))\n",
    "    accuracy = 1.0 * NB_P_O_test.filter(lambda x: x[0] == x[1]).count() / labeled_testing_data.count()\n",
    "    print(\"Testing Data\",accuracy)\n",
    "    \n",
    "    # Instantiate metrics object\n",
    "    metrics = MulticlassMetrics(NB_P_O_test)\n",
    "    \n",
    "    # Overall statistics\n",
    "    precision = metrics.precision()\n",
    "    recall = metrics.recall()\n",
    "    f1Score = metrics.fMeasure()\n",
    "    confMatrix=metrics.confusionMatrix().toArray()\n",
    "    print(confMatrix)\n",
    "    print(\"Summary Stats\")\n",
    "    print(\"Precision = %s\" % precision)\n",
    "    print(\"Recall = %s\" % recall)\n",
    "    print(\"F1 Score = %s\" % f1Score)\n",
    "    # Statistics by class\n",
    "    labels = [0.0,1.0]\n",
    "    for label in sorted(labels):\n",
    "        print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "        print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "        print(\"Class %s F1 Measure = %s\\n\\n\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "    metrics=BinaryClassificationMetrics(NB_P_O_test)\n",
    "    print(\"Area under ROC =%s\"% metrics.areaUnderROC)\n",
    "\n",
    "    ### End of Naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram implementation of naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Building model on Train Data...\n",
      "[LabeledPoint(0.0, (50000,[1657,4556,10298,10973,31939,35220,46417,49002],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])), LabeledPoint(0.0, (50000,[8311,11502,28608,47002],[1.0,1.0,1.0,1.0]))]\n",
      "Training Data 0.8152375\n",
      "Testing Data 0.5793871866295265\n",
      "[[  92.   85.]\n",
      " [  66.  116.]]\n",
      "Summary Stats\n",
      "Precision = 0.5793871866295265\n",
      "Recall = 0.5793871866295265\n",
      "F1 Score = 0.5793871866295265\n",
      "Class 0.0 precision = 0.5822784810126582\n",
      "Class 0.0 recall = 0.519774011299435\n",
      "Class 0.0 F1 Measure = 0.5492537313432835\n",
      "\n",
      "\n",
      "Class 1.0 precision = 0.5771144278606966\n",
      "Class 1.0 recall = 0.6373626373626373\n",
      "Class 1.0 F1 Measure = 0.6057441253263708\n",
      "\n",
      "\n",
      "Area under ROC =0.5785683243310362\n"
     ]
    }
   ],
   "source": [
    "    hashingTF = HashingTF(50000)\n",
    "    ### Start of Naive bayes\n",
    "\n",
    "    print('## Building model on Train Data...')\n",
    "    \n",
    "    labeled_training_data = biGramfunc(train_data).map(lambda x: LabeledPoint(x[1],hashingTF.transform(x[0])))\n",
    "    labeled_training_data.persist()\n",
    "    print(labeled_training_data.take(2))\n",
    "    labeled_testing_data =biGramfunc(test_data).map(lambda x: LabeledPoint(x[1],hashingTF.transform(x[0])))\n",
    "    labeled_testing_data.persist()\n",
    "    \n",
    "    NBmodel = NaiveBayes.train(labeled_training_data,lambda_=2.0)\n",
    "    \n",
    "    \n",
    "    NB_P_O_train = labeled_training_data.map(lambda p: (NBmodel.predict(p.features), p.label))\n",
    "    accuracy = 1.0 * NB_P_O_train.filter(lambda x: x[0] == x[1]).count() / labeled_training_data.count()\n",
    "    print(\"Training Data\",accuracy)\n",
    "    #\n",
    "    NB_P_O_test = labeled_testing_data.map(lambda p:(float(NBmodel.predict(p.features)), p.label))\n",
    "    accuracy = 1.0 * NB_P_O_test.filter(lambda x: x[0] == x[1]).count() / labeled_testing_data.count()\n",
    "    print(\"Testing Data\",accuracy)\n",
    "    \n",
    "    # Instantiate metrics object\n",
    "    metrics = MulticlassMetrics(NB_P_O_test)\n",
    "    \n",
    "    # Overall statistics\n",
    "    precision = metrics.precision()\n",
    "    recall = metrics.recall()\n",
    "    f1Score = metrics.fMeasure()\n",
    "    confMatrix=metrics.confusionMatrix().toArray()\n",
    "    print(confMatrix)\n",
    "    print(\"Summary Stats\")\n",
    "    print(\"Precision = %s\" % precision)\n",
    "    print(\"Recall = %s\" % recall)\n",
    "    print(\"F1 Score = %s\" % f1Score)\n",
    "    # Statistics by class\n",
    "    labels = [0.0,1.0]\n",
    "    for label in sorted(labels):\n",
    "        print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "        print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "        print(\"Class %s F1 Measure = %s\\n\\n\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "    metrics=BinaryClassificationMetrics(NB_P_O_test)\n",
    "    print(\"Area under ROC =%s\"% metrics.areaUnderROC)\n",
    "\n",
    "    ### End of Naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performed TF IDF on both training and test data and supplied it to Logistic Regression Stochastic Gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Calculating TF*IDF for Train Data...\n",
      "## Creating Labeled points for Train Data...\n",
      "## Calculating TF*IDF for Test Data...\n",
      "## Creating Labeled points for Test Data...\n",
      "Training Data 0.7561375\n",
      "testing Data 0.766016713091922\n",
      "[[ 129.   48.]\n",
      " [  36.  146.]]\n",
      "Summary Stats\n",
      "Precision = 0.766016713091922\n",
      "Recall = 0.766016713091922\n",
      "F1 Score = 0.766016713091922\n",
      "Class 0.0 precision = 0.7818181818181819\n",
      "Class 0.0 recall = 0.7288135593220338\n",
      "Class 0.0 F1 Measure = 0.7543859649122807\n",
      "\n",
      "\n",
      "Class 1.0 precision = 0.7525773195876289\n",
      "Class 1.0 recall = 0.8021978021978022\n",
      "Class 1.0 F1 Measure = 0.7765957446808511\n",
      "\n",
      "\n",
      "Area under ROC =0.7655056807599181\n"
     ]
    }
   ],
   "source": [
    "    print('## Calculating TF*IDF for Train Data...')\n",
    "    train_tf = train_data.map(lambda x: hashingTF.transform(x[0]))\n",
    "    train_tf.cache()\n",
    "    idf = IDF().fit(train_tf)\n",
    "    train_tfidf = idf.transform(train_tf)\n",
    "    #train_tf.unpersist()\n",
    "    print('## Creating Labeled points for Train Data...')\n",
    "    tfidf_index = train_tfidf.zipWithIndex()\n",
    "    training_index = train_data.zipWithIndex()\n",
    "    index_training = training_index.map(lambda line: (line[1], line[0][1]))#label,\n",
    "    index_tfidf = tfidf_index.map(lambda l: (l[1], l[0]))#index,vector\n",
    "    joined_tfidf_training = index_training.join(index_tfidf).map(lambda x: x[1])#label,vector\n",
    "    # print(training_labeled.take(2))\n",
    "    labeled_training_data = joined_tfidf_training.map(lambda k: LabeledPoint(k[0], k[1]))\n",
    "    # print(labeled_training_data.take(2))\n",
    "    labeled_training_data.cache()\n",
    "    #train_data.unpersist()\n",
    "\n",
    "\n",
    "\n",
    "    print('## Calculating TF*IDF for Test Data...')\n",
    "    test_tf = test_data.map(lambda x: hashingTF.transform(x[0]))\n",
    "    test_tf.cache()\n",
    "    #idf = IDF().fit(test_tf)\n",
    "    test_tfidf = idf.transform(test_tf)\n",
    "    test_tf.unpersist()\n",
    "    print('## Creating Labeled points for Test Data...')\n",
    "    tfidf_index = test_tfidf.zipWithIndex()\n",
    "    testing_index = test_data.zipWithIndex()\n",
    "    index_testing = testing_index.map(lambda line: (line[1], line[0][1]))\n",
    "    index_tfidf = tfidf_index.map(lambda l: (l[1], l[0]))\n",
    "    joined_tfidf_testing = index_testing.join(index_tfidf).map(lambda x: x[1])\n",
    "    # print(testing_labeled.take(2))\n",
    "    labeled_testing_data = joined_tfidf_testing.map(lambda k: LabeledPoint(k[0], k[1]))\n",
    "    # print(labeled_testing_data.take(2))\n",
    "    labeled_testing_data.cache()\n",
    "    #test_data.unpersist()\n",
    "\n",
    "    LRmodel = LogisticRegressionWithSGD.train(labeled_training_data)\n",
    "    #\n",
    "    LRmodel.setThreshold(0.5)\n",
    "    LR_P_O_train = labeled_training_data.map(lambda p: (LRmodel.predict(p.features), p.label))\n",
    "    accuracy = 1.0 * LR_P_O_train.filter(lambda x: x[1] == x[0]).count() / labeled_training_data.count()\n",
    "    print(\"Training Data\",accuracy)\n",
    "    #\n",
    "    LR_P_O_test = labeled_testing_data.map(lambda p: (float(LRmodel.predict(p.features)), p.label))\n",
    "    accuracy = 1.0 * LR_P_O_test.filter(lambda x: x[1] == x[0]).count() / labeled_testing_data.count()\n",
    "    print(\"testing Data\",accuracy)\n",
    "    \n",
    "    # Instantiate metrics object\n",
    "    metrics = MulticlassMetrics(LR_P_O_test)\n",
    "    \n",
    "    # Overall statistics\n",
    "    precision = metrics.precision()\n",
    "    recall = metrics.recall()\n",
    "    f1Score = metrics.fMeasure()\n",
    "    confMatrix=metrics.confusionMatrix().toArray()\n",
    "    print(confMatrix)\n",
    "    print(\"Summary Stats\")\n",
    "    print(\"Precision = %s\" % precision)\n",
    "    print(\"Recall = %s\" % recall)\n",
    "    print(\"F1 Score = %s\" % f1Score)\n",
    "    # Statistics by class\n",
    "    labels = [0.0,1.0]\n",
    "    for label in sorted(labels):\n",
    "        print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "        print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "        print(\"Class %s F1 Measure = %s\\n\\n\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "    metrics=BinaryClassificationMetrics(LR_P_O_test)\n",
    "    print(\"Area under ROC =%s\"% metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supplied TF IDf vector to decision trees and computed the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = DecisionTree.trainClassifier(labeled_training_data, numClasses=2, categoricalFeaturesInfo={},impurity='gini', maxDepth=30, maxBins=100)\n",
    "predictions = model.predict(labeled_training_data.map(lambda x: x.features))\n",
    "labelsAndPredictions = labeled_training_data.map(lambda lp: lp.label).zip(predictions)\n",
    "train_accuracy = labelsAndPredictions.filter(lambda v: v[0] == v[1]).count() / float(train_data.count())\n",
    "print(\"Train accuracy is %s\" % round(float(train_accuracy), 4))\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(labeled_testing_data.map(lambda x: x.features))\n",
    "labelsAndPredictions = labeled_testing_data.map(lambda lp: lp.label).zip(predictions)\n",
    "test_accuracy = labelsAndPredictions.filter(lambda v: v[0] == v[1]).count() / float(test_data.count())\n",
    "print(\"Test accuracy is %s\" % round(float(test_accuracy), 4))\n",
    "metrics = MulticlassMetrics(labelsAndPredictions)\n",
    "    \n",
    "    # Overall statistics\n",
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1Score = metrics.fMeasure()\n",
    "confMatrix=metrics.confusionMatrix().toArray()\n",
    "print(confMatrix)\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)\n",
    "    # Statistics by class\n",
    "labels = [0.0,1.0]\n",
    "for label in sorted(labels):\n",
    "    print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "    print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "    print(\"Class %s F1 Measure = %s\\n\\n\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "metrics=BinaryClassificationMetrics(labelsAndPredictions)\n",
    "print(\"Area under ROC =%s\"% metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioned the training data into 10 splits and tested the classifier accuracy with the remaining single partition.\n",
    "data=labeled_training_data--For Logistic\n",
    "data=train_tf---for NB\n",
    "data=labeled_training_data--For Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = labeled_training_data\n",
    "data.cache()\n",
    "training =[]\n",
    "val = []\n",
    "partitions = data.randomSplit([.1,.1,.1,.1,.1,.1,.1,.1,.1,.1],seed = 5)\n",
    "for k in range(0,10):\n",
    "    if k is 0:\n",
    "        training.append(sc.union(partitions[1:10]))\n",
    "    elif k is 9:\n",
    "        training.append(sc.union(partitions[0:9]))\n",
    "    else:\n",
    "        p1 = sc.union(partitions[0:k])\n",
    "        p2 = sc.union(partitions[k+1:10])    \n",
    "        training.append(p1+p2)\n",
    "    val.append(partitions[k])\n",
    "    print(training[k].count(),val[k].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracyLG = []\n",
    "for k in range(0,10):    \n",
    "    model = LogisticRegressionWithSGD.train(training[k])\n",
    "    # Make prediction and test accuracy.\n",
    "    predictionAndLabel = val[k].map(lambda p: (model.predict(p.features), p.label))\n",
    "    #print(predictionAndLabel.take(3))\n",
    "    accuracyLG.append(1.0 * predictionAndLabel.filter(lambda xv: xv[1] == xv[0]).count() / val[k].count())\n",
    "    #print(accuracyLG[k])\n",
    "\n",
    "avg_accuracyLG = sum(accuracyLG)/len(accuracyLG)\n",
    "print(avg_accuracyLG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracyBNB = []\n",
    "# Train a naive Bayes model.\n",
    "for k in range(0,10):\n",
    "    model= NaiveBayes.train(training[k], 1.0)\n",
    "    # Make prediction and test accuracy.\n",
    "    predictionAndLabel = val[k].map(lambda p: (model.predict(p.features), p.label))\n",
    "    #print(predictionAndLabel.take(3))\n",
    "    accuracyBNB.append(1.0 * predictionAndLabel.filter(lambda xv: xv[1] == xv[0]).count() / val[k].count())\n",
    "    #print(accuracyBNB[k])\n",
    "\n",
    "avg_accuracyBNB = sum(accuracyBNB)/len(accuracyBNB)\n",
    "print(\"Avg accuracy for Naive Bayes:%s\"% avg_accuracyBNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "#model = []\n",
    "for k in range(0,10):    \n",
    "    model = DecisionTree.trainClassifier(training[k], numClasses=2, categoricalFeaturesInfo={},impurity='gini', maxDepth=30, maxBins=100)\n",
    "    predictions = model.predict(val[k].map(lambda x: x.features))\n",
    "    labelsAndPredictions = val[k].map(lambda lp: lp.label).zip(predictions)\n",
    "    accuracy.append(labelsAndPredictions.filter(lambda v: v[0] == v[1]).count() / float(val[k].count()))\n",
    "    #print(accuracy[k])\n",
    "\n",
    "avg_accuracy = sum(accuracy)/len(accuracy)\n",
    "print(\"Avg accuracy for Decision Trees:%s\" % avg_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy plot for three classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "print ('Success')\n",
    "N = 3\n",
    "TrainingAcc = (80.56, 75.72, 70.99)\n",
    "\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.25       # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, TrainingAcc, width, color='r')\n",
    "ax.set_ylim([50,100])\n",
    "KfoldAcc = (74.5, 74.38, 67.22)\n",
    "rects2 = ax.bar(ind + width, KfoldAcc, width, color='y')\n",
    "\n",
    "TestAcc = (81.05, 76.88, 70.47)\n",
    "rects3 = ax.bar(ind + 2*width, TestAcc, width, color='b')\n",
    "\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Accuracy in %')\n",
    "ax.set_title('Accuracies of NB , LOG and DT Classifiers')\n",
    "ax.set_xticks(ind + width)\n",
    "ax.set_xticklabels(('NB', 'LOG', 'DT'))\n",
    "\n",
    "ax.legend((rects1[0], rects2[0], rects3[0]), ('Training', '10-FoldCV', 'Test'))\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    # attach some text labels\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.02*height,\n",
    "                '%d' % int(height),\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "plt.savefig('AccuracyPlot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roc curve plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=[1,0.74,0.50,0.271,0.135,0.039,0]\n",
    "y=[1,0.98,0.93,0.80,0.63,0.38,0]\n",
    "plt.plot(x,y,marker='o')\n",
    "plt.plot([0,1],[0,1], ls='dotted')\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.suptitle('ROC Curve', fontsize=14, fontweight='bold')\n",
    "plt.savefig('ROC123.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting correct and incorrect tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('## Building model on Train Data...')\n",
    "\n",
    "labeled_training_data = train_data.map(lambda x: LabeledPoint(x[1],hashingTF.transform(x[0])))\n",
    "labeled_training_data.persist()\n",
    "print(labeled_training_data.take(2))\n",
    "labeled_testing_data = test_data.map(lambda x: LabeledPoint(x[1],hashingTF.transform(x[0])))\n",
    "labeled_testing_data.persist()\n",
    "\n",
    "NBmodel = NaiveBayes.train(labeled_training_data,lambda_=1.0)\n",
    "\n",
    "NB_P_O_train = labeled_training_data.map(lambda p: (NBmodel.predict(p.features), p.label))\n",
    "accuracy = 1.0 * NB_P_O_train.filter(lambda x: x[0] == x[1]).count() / labeled_training_data.count()\n",
    "print(\"Training Data %s\" % accuracy)\n",
    "predictions = NBmodel.predict(labeled_testing_data.map(lambda x: x.features))\n",
    "NB_P_O_test = labeled_testing_data.map(lambda lp:  lp.label).zip(predictions)\n",
    "#\n",
    "accuracy = 1.0 * NB_P_O_test.filter(lambda x: x[0] == x[1]).count() / labeled_testing_data.count()\n",
    "print(\"Testing Data %s \" % accuracy)\n",
    "\n",
    "NBmodel.save(sc, \"NBModel\")\n",
    "NB_sameModel = NaiveBayesModel.load(sc, \"NBModel\")\n",
    "j = predictions.coalesce(int(test_data.getNumPartitions()))\n",
    "temp_testpred = test_data.zip(j)\n",
    "# print(temp_testpred.take(2))\n",
    "correctly_classified = temp_testpred.filter(lambda x: x[0][1] == x[1])\n",
    "print(correctly_classified.take(7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
